{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9395c475",
   "metadata": {},
   "source": [
    "## DSC 180AB Data Science Capstone\n",
    "### Replication Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49e865",
   "metadata": {},
   "source": [
    "Team Members:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd0b66-89d3-4e62-a3af-1b62b3dbaf27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 1.3 Methodology \n",
    "\n",
    "For each dataset, the sensitive attribute is 'RACE' constructed as follows: 'Whites' (privileged class) defined by the features RACEV2X = 1 (White) and HISPANX = 2 (non Hispanic); 'Non-Whites' that included everyone else.  \n",
    "\n",
    "* Along with race as the sensitive feature, other features used for modeling include demographics  (such as age, gender, active duty status), physical/mental health assessments, diagnosis codes (such as history of diagnosis of cancer, or diabetes), and limitations (such as cognitive or hearing or vision limitation).\n",
    "* To measure utilization, a composite feature, 'UTILIZATION', was created to measure the total number of trips requiring some sort of medical care by summing up the following features: OBTOTV15(16), the number of office based visits;  OPTOTV15(16), the number of outpatient visits; ERTOT15(16), the number of ER visits;  IPNGTD15(16), the number of inpatient nights, and  + HHTOTD16, the number of home health visits.\n",
    "* The model classification task is to predict whether a person would have 'high' utilization (defined as UTILIZATION >= 10, roughly the average utilization for the considered population). High utilization respondents constituted around 17% of each dataset.\n",
    "* To simulate the scenario, each dataset is split into 3 parts: a train, a validation, and a test/deployment part.\n",
    "\n",
    "**We assume that the model is initially built and tuned using the 2015 Panel 19 train/test data**\n",
    "* It is then put into practice and used to score people to identify potential candidates for care management. \n",
    "* Initial deployment is simulated to 2015 Panel 20 deployment data. \n",
    "* To show change in performance and/or fairness over time, the 2016 Panel 21 deployment data is used. \n",
    "* Finally, if drift is observed, the 2015 train/validation data is used to learn a new model and evaluated again on the 2016 deployment data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef7f4e6-b2f0-4c7c-ab1f-9094a6339865",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----\n",
    "# Replication Project Part 03b, Discussion Section: Bias Mitigation Techniques\n",
    "\n",
    "## Refer to [Replication Project Part 03a](link here) for the initial code and development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed5dfc-0e72-47b7-ba81-a36abe98b40c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b3e294",
   "metadata": {},
   "source": [
    "## [5.](#Table-of-Contents) Bias Mitigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d267d787-ca80-486a-a183-703e54c58ab9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [5A.](#Table-of-Contents) Bias mitigation using pre-processing technique, Reweighing - AIF360 Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956822fc-281e-4f7e-ad3c-ebdacab682c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### [5B.](#Table-of-Contents) Prejudice Remover (in-processing bias mitigation) -  AIF360 Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff534f6-bbf7-4e77-ba99-15e24c43e784",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### [5C.](#Table-of-Contents) Bias mitigation using a technique of your own\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6453816-0a2a-4ec2-9d7c-bb5e73141119",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64133381-263e-4569-bd34-fb8b3c5ba945",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 5 Discussion\n",
    "\n",
    "Use these questions to evaluate your models during bias-mitigation processes. \n",
    "\n",
    "### 5A. For **both** the logistic regression and random forest classifiers, please include visualizations the pre-processing results of your bias mitigation techniques. \n",
    "\n",
    "**In addition, for each Model + Bias mitigation technique, please write 1-2 SENTENCES explaining the following:**\n",
    "\n",
    "1. Describe the bias mitigation techniques applied (what stage? type? explain what that debiasing technique does?) \n",
    "2. Based on defintions and sources of bias we covered in class, what type of bias are we trying to mitigate in our models?\n",
    "3. Do both models exhibit fairness and maintain accuracy? List the fairness metrics you used to support this. What about model drift?\n",
    "4. For the classifier ‘high’ utilization in these models + pre-processing mitigation technique; would you recommend this for our use case as a \"fair\" classifier? Why or why not? Use previous questions and the slide 'Sources of Bias in AI and Health Data' from our Week-07 slides, to help you answer this. \n",
    "\n",
    "\n",
    "### 5B. For **both** the logistic regression and random forest classifiers, please include visualizations the post-processing results of your bias mitigation techniques. \n",
    "\n",
    "**In addition, for each Model + Bias mitigation technique, please write 1-2 SENTENCES explaining the following:**\n",
    "\n",
    "1. Describe the bias mitigation techniques applied (what stage? type? explain what that debiasing technique does?) \n",
    "2. Based on defintions and sources of bias we covered in class, what type of bias are we trying to mitigate in our models?\n",
    "3. Do both models exhibit fairness and maintain accuracy? List the fairness metrics you used to support this. What about model drift?\n",
    "4. For the classifier ‘high’ utilization in these models + post-processing mitigation technique; would you recommend this for our use case as a \"fair\" classifier? Why or why not? Use previous questions and the slide 'Sources of Bias in AI and Health Data' from our Week-07 slides, to help you answer this. \n",
    "\n",
    "### 5C. For **both** the logistic regression and random forest classifiers, please include visualizations for processing results of your bias mitigation techniques OF YOUR CHOICE. \n",
    "\n",
    "**In addition, for each Model + Bias mitigation technique of your choice, please write 1-2 SENTENCES explaining the following:**\n",
    "\n",
    "1. Describe the bias mitigation techniques applied (what stage? type? explain what that debiasing technique does?) \n",
    "2. Based on defintions and sources of bias we covered in class, what type of bias are we trying to mitigate in our models?\n",
    "3. Do both models exhibit fairness and maintain accuracy? List the fairness metrics you used to support this. What about model drift?\n",
    "4. For the classifier ‘high’ utilization in these models + post-processing mitigation technique; would you recommend this for our use case as a \"fair\" classifier? Why or why not? Use previous questions and the slide 'Sources of Bias in AI and Health Data' from our Week-07 slides, to help you answer this.\n",
    "\n",
    "## Section 5: Overall Discussion for Bias Mitigation, write 1-2 paragraphs for each question\n",
    "\n",
    "1. What factors must be considered during AI model-development and performance? How and where are they vulnerable to introducing bias?\n",
    "2. How and what should be measured to assess downstream impact of AI, and what factors should be used to audit for bias and clinical impact? \n",
    "3. Define what disparate impact is. How can the type, dimension, collection-method, and representation in data lead to bias and disparate impact in communities of concern? Use the MEPS codebooks, and websites to help you explain this. Refer to Center of Disease Control (CDC) [Health Equity Guiding Principles for Inclusive Communication](https://www.cdc.gov/healthcommunication/Health_Equity.html) for style recommendations when referring to affected groups. \n",
    "4. Where there any social factors overlooked when developing AI-targets or outcomes for the AIF360 exmaple? How could they delay access and quality of care to underserved populations?\n",
    "5. (a) Overall, if you were to select ONE (a) Model, and (b) Bias-mitigation technique - for this use case where you are recommending a 'Fair' classifier for flagging 'High' utilization, which would you chose and why? (b) How would it affect Non-White beneficiaries who could have risk factor predictors that could inform a model to prioritize additional care? How could it do the opposite? (c) Explain how you would justify fair-accuracy trade-off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9389bce9-ad38-4b91-80f6-fdef6239856e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
